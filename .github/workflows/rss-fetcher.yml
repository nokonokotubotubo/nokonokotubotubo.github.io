# .github/workflows/rss-fetcher.yml (æ­£ã—ã„ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆå¯¾å¿œç‰ˆ)
name: Custom RSS Feed Fetcher

on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

jobs:
  fetch-rss:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install feedparser==6.0.11 requests beautifulsoup4 lxml python-dateutil

    - name: Create directory structure
      run: |
        mkdir -p data  # ãƒ«ãƒ¼ãƒˆã® data ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª

    - name: Execute Custom RSS Fetcher
      run: |
        python3 << 'EOF'
        import feedparser
        import requests
        import json
        import os
        from datetime import datetime, timezone
        import time
        from pathlib import Path
        from typing import cast
        from feedparser import FeedParserDict
        import re
        from dateutil import parser as date_parser

        class CustomRSSFetcher:
            def __init__(self):
                self.headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept': 'application/rss+xml, application/xml, text/xml, */*',
                    'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache',
                    'Connection': 'keep-alive'
                }
                
                self.rss_feeds = [
                    {
                        'id': 'nhk-news',
                        'name': 'NHKãƒ‹ãƒ¥ãƒ¼ã‚¹',
                        'url': 'https://www3.nhk.or.jp/rss/news/cat0.xml',
                        'description': 'NHK ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                        'color': '#2E7D32'
                    },
                    {
                        'id': 'gigazine',
                        'name': 'GIGAZINE',
                        'url': 'https://gigazine.net/news/rss_2.0/',
                        'description': 'GIGAZINE ãƒ†ãƒƒã‚¯è¨˜äº‹',
                        'color': '#FF4081'
                    },
                    {
                        'id': 'yahoo-news',
                        'name': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹', 
                        'url': 'https://news.yahoo.co.jp/rss/topics/top-picks.xml',
                        'description': 'Yahoo! ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                        'color': '#6F42C1'
                    },
                    {
                        'id': 'itmedia-news',
                        'name': 'ITmedia',
                        'url': 'https://rss.itmedia.co.jp/rss/2.0/topstory.xml',
                        'description': 'ITmedia ãƒˆãƒƒãƒ—ã‚¹ãƒˆãƒ¼ãƒªãƒ¼',
                        'color': '#4A90E2'
                    },
                    {
                        'id': 'reuters-tech',
                        'name': 'Reuters Tech',
                        'url': 'https://feeds.reuters.com/reuters/technologyNews',
                        'description': 'Reuters ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                        'color': '#FF6B35'
                    }
                ]
                
                # ä¿®æ­£: ãƒ«ãƒ¼ãƒˆã®dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
                self.output_dir = Path('data')
                self.output_dir.mkdir(parents=True, exist_ok=True)
                
                self.processed_links_file = self.output_dir / 'processed_links.json'
                self.load_processed_links()

            def load_processed_links(self):
                """å‡¦ç†æ¸ˆã¿ãƒªãƒ³ã‚¯ã®èª­ã¿è¾¼ã¿"""
                try:
                    if self.processed_links_file.exists():
                        with self.processed_links_file.open() as f:
                            self.processed_links = set(json.load(f))
                    else:
                        self.processed_links = set()
                except Exception as e:
                    print(f"âš ï¸ å‡¦ç†æ¸ˆã¿ãƒªãƒ³ã‚¯èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    self.processed_links = set()

            def save_processed_links(self):
                """å‡¦ç†æ¸ˆã¿ãƒªãƒ³ã‚¯ã®ä¿å­˜"""
                try:
                    with self.processed_links_file.open('w') as f:
                        json.dump(list(self.processed_links), f, indent=2)
                except Exception as e:
                    print(f"âš ï¸ å‡¦ç†æ¸ˆã¿ãƒªãƒ³ã‚¯ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

            def fetch_rss_feed(self, feed_config):
                """å€‹åˆ¥RSSãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—"""
                try:
                    print(f"ğŸ“¡ å–å¾—é–‹å§‹: {feed_config['name']} ({feed_config['url']})")
                    
                    session = requests.Session()
                    session.headers.update(self.headers)
                    
                    response = session.get(feed_config['url'], timeout=30)
                    response.raise_for_status()
                    
                    print(f"âœ… HTTP {response.status_code}: {feed_config['name']}")
                    
                    fpdict = cast(FeedParserDict, feedparser.parse(response.content))
                    
                    if fpdict.bozo:
                        print(f"âš ï¸ {feed_config['name']}: è»½å¾®ãªãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ (bozo={fpdict.bozo})")
                        if fpdict.bozo_exception:
                            print(f"   è©³ç´°: {fpdict.bozo_exception}")
                    
                    fetched_entries = cast(list[FeedParserDict], fpdict.get("entries", []))
                    
                    if not fetched_entries:
                        print(f"âš ï¸ {feed_config['name']}: ã‚¨ãƒ³ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        return None
                    
                    print(f"ğŸ“Š {feed_config['name']}: {len(fetched_entries)}ä»¶ã®ã‚¨ãƒ³ãƒˆãƒªã‚’æ¤œå‡º")
                    
                    processed_entries = []
                    new_entries_count = 0
                    
                    for entry in fetched_entries[:10]:
                        try:
                            entry_link = entry.get('link', '')
                            if entry_link in self.processed_links:
                                continue
                            
                            processed_entry = self.process_entry(entry, feed_config)
                            if processed_entry:
                                processed_entries.append(processed_entry)
                                self.processed_links.add(entry_link)
                                new_entries_count += 1
                                
                        except Exception as e:
                            print(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒªå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                            continue
                    
                    feed_data = {
                        'title': fpdict.feed.get('title', feed_config['name']),
                        'link': fpdict.feed.get('link', ''),
                        'description': fpdict.feed.get('description', feed_config['description']),
                        'generator': fpdict.feed.get('generator', ''),
                        'language': fpdict.feed.get('language', 'ja'),
                        'published': datetime.now(timezone.utc).isoformat(),
                        'entries': processed_entries[:5]
                    }
                    
                    print(f"âœ… {feed_config['name']}: {len(processed_entries)}ä»¶å‡¦ç†å®Œäº† (æ–°è¦: {new_entries_count}ä»¶)")
                    return feed_data
                    
                except requests.exceptions.RequestException as e:
                    print(f"âŒ {feed_config['name']}: ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ - {e}")
                    return None
                except Exception as e:
                    print(f"âŒ {feed_config['name']}: å‡¦ç†ã‚¨ãƒ©ãƒ¼ - {e}")
                    return None

            def process_entry(self, entry, feed_config):
                """å€‹åˆ¥ã‚¨ãƒ³ãƒˆãƒªã®å‡¦ç†"""
                try:
                    title = entry.get('title', 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜')
                    link = entry.get('link', '')
                    
                    description = (
                        entry.get('description', '') or 
                        entry.get('summary', '') or 
                        entry.get('content', [{}])[0].get('value', '') if entry.get('content') else ''
                    )
                    
                    published_date = self.parse_date(
                        entry.get('published', '') or 
                        entry.get('pubDate', '') or 
                        entry.get('updated', '')
                    )
                    
                    thumbnail = self.extract_thumbnail(entry, description)
                    entry_id = entry.get('id', '') or entry.get('guid', '') or link
                    
                    return {
                        'id': entry_id,
                        'title': self.clean_text(title),
                        'link': link,
                        'description': self.create_summary(description),
                        'published': published_date,
                        'thumbnail': thumbnail,
                        'source': feed_config['name']
                    }
                    
                except Exception as e:
                    print(f"âš ï¸ ã‚¨ãƒ³ãƒˆãƒªå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    return None

            def parse_date(self, date_string):
                """æ—¥ä»˜æ–‡å­—åˆ—ã®è§£æ"""
                if not date_string:
                    return datetime.now(timezone.utc).isoformat()
                
                try:
                    parsed_date = date_parser.parse(date_string)
                    if parsed_date.tzinfo is None:
                        parsed_date = parsed_date.replace(tzinfo=timezone.utc)
                    return parsed_date.isoformat()
                except Exception as e:
                    print(f"âš ï¸ æ—¥ä»˜è§£æã‚¨ãƒ©ãƒ¼ ({date_string}): {e}")
                    return datetime.now(timezone.utc).isoformat()

            def extract_thumbnail(self, entry, description):
                """ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã®æŠ½å‡º"""
                thumbnail = ''
                
                if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                    thumbnail = entry.media_thumbnail[0].get('url', '')
                elif hasattr(entry, 'media_content') and entry.media_content:
                    thumbnail = entry.media_content[0].get('url', '')
                elif hasattr(entry, 'enclosures'):
                    for enclosure in entry.enclosures:
                        if enclosure.get('type', '').startswith('image/'):
                            thumbnail = enclosure.get('href', '') or enclosure.get('url', '')
                            break
                
                if not thumbnail and description:
                    img_patterns = [
                        r'<img[^>]+src=["\'](https?://[^"\']+)["\']/?>',
                        r'https?://[^\s<>"]+\.(?:jpg|jpeg|png|gif|webp|svg)',
                    ]
                    
                    for pattern in img_patterns:
                        match = re.search(pattern, description, re.IGNORECASE)
                        if match:
                            thumbnail = match.group(1) if 'src=' in pattern else match.group(0)
                            break
                
                return thumbnail

            def clean_text(self, text):
                """ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
                if not text:
                    return ''
                
                text = text.replace('&lt;', '<').replace('&gt;', '>').replace('&amp;', '&')
                text = text.replace('&quot;', '"').replace('&#39;', "'").replace('&apos;', "'")
                text = re.sub(r'\s+', ' ', text).strip()
                
                return text

            def create_summary(self, description, max_length=150):
                """èª¬æ˜æ–‡ã‹ã‚‰ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ"""
                if not description:
                    return 'èª¬æ˜ãŒã‚ã‚Šã¾ã›ã‚“'
                
                text = re.sub(r'<[^>]+>', '', description)
                text = self.clean_text(text)
                
                if len(text) <= max_length:
                    return text
                
                sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\.\!\?]', text)
                result = ''
                
                for sentence in sentences:
                    if len(result + sentence) <= max_length:
                        result += sentence + ('ã€‚' if sentence and not sentence.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')) else '')
                    else:
                        break
                
                if not result:
                    result = text[:max_length] + '...'
                
                return result

            def run(self):
                """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
                print("ğŸš€ ã‚«ã‚¹ã‚¿ãƒ RSSãƒ•ã‚§ãƒƒãƒãƒ£ãƒ¼é–‹å§‹")
                print(f"ğŸ“‚ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.output_dir} (ãƒ«ãƒ¼ãƒˆ/data)")
                
                successful_feeds = []
                total_articles = 0
                
                for feed_config in self.rss_feeds:
                    try:
                        feed_data = self.fetch_rss_feed(feed_config)
                        
                        if feed_data and feed_data['entries']:
                            # ãƒ«ãƒ¼ãƒˆã®dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
                            output_file = self.output_dir / f"{feed_config['id']}.json"
                            
                            with output_file.open('w', encoding='utf-8') as f:
                                json.dump(feed_data, f, ensure_ascii=False, indent=2)
                            
                            successful_feeds.append({
                                'id': feed_config['id'],
                                'name': feed_config['name'],
                                'description': feed_config['description'],
                                'file': f"{feed_config['id']}.json",
                                'color': feed_config['color'],
                                'articleCount': len(feed_data['entries'])
                            })
                            
                            total_articles += len(feed_data['entries'])
                            print(f"ğŸ’¾ {feed_config['name']}: ãƒ«ãƒ¼ãƒˆ/dataã«ä¿å­˜å®Œäº†")
                        
                    except Exception as e:
                        print(f"âŒ {feed_config['name']}: äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ - {e}")
                    
                    time.sleep(2)
                
                self.save_processed_links()
                self.generate_metadata(successful_feeds, total_articles)
                
                print(f"\nğŸ“Š å®Ÿè¡Œçµæœã‚µãƒãƒªãƒ¼:")
                print(f"   âœ… æˆåŠŸ: {len(successful_feeds)}ãƒ•ã‚£ãƒ¼ãƒ‰")
                print(f"   ğŸ“° ç·è¨˜äº‹æ•°: {total_articles}ä»¶")
                print(f"   ğŸ“ å‡¦ç†æ¸ˆã¿ãƒªãƒ³ã‚¯: {len(self.processed_links)}ä»¶")
                print(f"   ğŸ“‚ ä¿å­˜å…ˆ: data/ (ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª)")
                
                if len(successful_feeds) == 0:
                    print("âŒ å…¨ã¦ã®ãƒ•ã‚£ãƒ¼ãƒ‰ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")
                    exit(1)
                else:
                    print("ğŸ‰ RSSå–å¾—å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ")

            def generate_metadata(self, successful_feeds, total_articles):
                """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ"""
                metadata = {
                    'lastUpdated': datetime.now(timezone.utc).isoformat(),
                    'totalFeeds': len(successful_feeds),
                    'totalArticles': total_articles,
                    'processedLinks': len(self.processed_links),
                    'feeds': successful_feeds
                }
                
                # ãƒ«ãƒ¼ãƒˆã®dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜
                metadata_file = self.output_dir / 'feeds-meta.json'
                with metadata_file.open('w', encoding='utf-8') as f:
                    json.dump(metadata, f, ensure_ascii=False, indent=2)
                
                print(f"ğŸ“‹ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ: {metadata_file}")

        # å®Ÿè¡Œ
        if __name__ == '__main__':
            fetcher = CustomRSSFetcher()
            fetcher.run()
        EOF

    - name: Validate Generated Files
      run: |
        echo "=== ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼ (data/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª) ==="
        total_files=0
        valid_files=0
        
        for file in data/*.json; do
          if [ -f "$file" ]; then
            ((total_files++))
            filename=$(basename "$file")
            
            if jq empty "$file" 2>/dev/null; then
              ((valid_files++))
              
              if [ "$filename" = "feeds-meta.json" ]; then
                feeds_count=$(jq '.totalFeeds' "$file")
                total_articles=$(jq '.totalArticles' "$file")
                echo "ğŸ“‹ $filename: ${feeds_count}ãƒ•ã‚£ãƒ¼ãƒ‰, ${total_articles}è¨˜äº‹"
              else
                entries=$(jq '.entries | length' "$file")
                title=$(jq -r '.title' "$file")
                echo "âœ… $filename: ${entries}ä»¶ - $title"
              fi
            else
              echo "âŒ $filename: JSONç„¡åŠ¹"
              head -3 "$file"
            fi
          fi
        done
        
        echo ""
        echo "ğŸ“Š æ¤œè¨¼çµæœ: $valid_files/$total_files ãƒ•ã‚¡ã‚¤ãƒ«ãŒæœ‰åŠ¹"
        echo "ğŸ“‚ ä¿å­˜å ´æ‰€: https://username.github.io/data/"

    - name: Commit and Push Changes
      uses: stefanzweifel/git-auto-commit-action@v4
      with:
        commit_message: 'RSS feeds updated via custom Python script - $(date -u +"%Y-%m-%d %H:%M UTC")'
        file_pattern: 'data/*.json'  # ãƒ«ãƒ¼ãƒˆã®dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        commit_user_name: 'Custom RSS Fetcher'
        commit_user_email: 'action@github.com'
