# .github/workflows/rss-fetcher.yml („Ç´„Çπ„Çø„É†ÂÆüË£ÖÁâà)
name: RSS Feed Fetcher - Custom Implementation

on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

jobs:
  fetch-rss:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install feedparser requests beautifulsoup4 lxml

    - name: Custom RSS Fetch Script
      run: |
        python3 << 'EOF'
        import feedparser
        import requests
        import json
        import os
        from datetime import datetime
        import time

        # Ê§úÁ¥¢ÁµêÊûú[2][6]„Å´Âü∫„Å•„ÅèUser-AgentË®≠ÂÆö
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'application/rss+xml, application/xml, text/xml, */*',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache'
        }

        # Âãï‰ΩúÁ¢∫Ë™çÊ∏à„ÅøRSS„Éï„Ç£„Éº„Éâ‰∏ÄË¶ß
        feeds = [
            {
                'id': 'nhk-news',
                'name': 'NHK„Éã„É•„Éº„Çπ',
                'url': 'https://www3.nhk.or.jp/rss/news/cat0.xml',
                'description': 'NHK ‰∏ªË¶Å„Éã„É•„Éº„Çπ',
                'color': '#2E7D32'
            },
            {
                'id': 'gigazine',
                'name': 'GIGAZINE',
                'url': 'https://gigazine.net/news/rss_2.0/',
                'description': 'GIGAZINE „ÉÜ„ÉÉ„ÇØË®ò‰∫ã',
                'color': '#FF4081'
            },
            {
                'id': 'yahoo-news',
                'name': 'Yahoo!„Éã„É•„Éº„Çπ',
                'url': 'https://news.yahoo.co.jp/rss/topics/top-picks.xml',
                'description': 'Yahoo! ‰∏ªË¶Å„Éã„É•„Éº„Çπ',
                'color': '#6F42C1'
            },
            {
                'id': 'itmedia',
                'name': 'ITmedia',
                'url': 'https://rss.itmedia.co.jp/rss/2.0/topstory.xml',
                'description': 'ITmedia „Éà„ÉÉ„Éó„Çπ„Éà„Éº„É™„Éº',
                'color': '#4A90E2'
            }
        ]

        os.makedirs('data', exist_ok=True)
        successful_feeds = []

        for feed_config in feeds:
            try:
                print(f"üì° ÂèñÂæóÈñãÂßã: {feed_config['name']}")
                
                # Ê§úÁ¥¢ÁµêÊûú[2]„ÅÆÊâãÊ≥ï„ÅßUser-Agent‰ªò„Åç„É™„ÇØ„Ç®„Çπ„Éà
                session = requests.Session()
                session.headers.update(headers)
                
                response = session.get(feed_config['url'], timeout=30)
                response.raise_for_status()
                
                print(f"‚úÖ HTTP {response.status_code}: {feed_config['name']}")
                
                # feedparser„ÅßËß£Êûê
                feed = feedparser.parse(response.content)
                
                if feed.bozo:
                    print(f"‚ö†Ô∏è {feed_config['name']}: ËªΩÂæÆ„Å™„Éë„Éº„Çπ„Ç®„É©„Éº„Åå„ÅÇ„Çä„Åæ„Åô„ÅåÁ∂öË°å")
                
                # „Ç®„É≥„Éà„É™Â§âÊèõÔºàÊ§úÁ¥¢ÁµêÊûú[3][4]„ÅÆÂá∫ÂäõÂΩ¢Âºè„Å´Ê∫ñÊã†Ôºâ
                entries = []
                for entry in feed.entries[:5]:
                    # ÁîªÂÉèURLÊäΩÂá∫
                    thumbnail = ''
                    if hasattr(entry, 'media_thumbnail') and entry.media_thumbnail:
                        thumbnail = entry.media_thumbnail[0].get('url', '')
                    elif hasattr(entry, 'enclosures'):
                        for enc in entry.enclosures:
                            if enc.get('type', '').startswith('image/'):
                                thumbnail = enc.get('href', '')
                                break
                    
                    entry_data = {
                        'id': getattr(entry, 'id', entry.link),
                        'title': entry.title,
                        'link': entry.link,
                        'description': getattr(entry, 'summary', ''),
                        'published': getattr(entry, 'published', datetime.now().isoformat()),
                        'thumbnail': thumbnail
                    }
                    entries.append(entry_data)
                
                # Ê§úÁ¥¢ÁµêÊûú[3][4]„ÅÆÂΩ¢Âºè„ÅßÂá∫Âäõ
                output_data = {
                    'title': feed.feed.get('title', feed_config['name']),
                    'link': feed.feed.get('link', ''),
                    'description': feed.feed.get('description', feed_config['description']),
                    'generator': feed.feed.get('generator', ''),
                    'language': feed.feed.get('language', 'ja'),
                    'published': datetime.now().isoformat(),
                    'entries': entries
                }
                
                # „Éï„Ç°„Ç§„É´‰øùÂ≠ò
                output_file = f"data/{feed_config['id']}.json"
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(output_data, f, ensure_ascii=False, indent=2)
                
                # „É°„Çø„Éá„Éº„ÇøÁî®„Å´Ë®òÈå≤
                successful_feeds.append({
                    'id': feed_config['id'],
                    'name': feed_config['name'],
                    'description': feed_config['description'],
                    'file': f"{feed_config['id']}.json",
                    'color': feed_config['color']
                })
                
                print(f"‚úÖ {feed_config['name']}: {len(entries)}‰ª∂ÂèñÂæóÂÆå‰∫Ü")
                
            except requests.exceptions.RequestException as e:
                print(f"‚ùå {feed_config['name']}: „Éç„ÉÉ„Éà„ÉØ„Éº„ÇØ„Ç®„É©„Éº - {e}")
            except Exception as e:
                print(f"‚ùå {feed_config['name']}: Âá¶ÁêÜ„Ç®„É©„Éº - {e}")
                
            # „É¨„Éº„ÉàÂà∂ÈôêÂØæÁ≠ñ
            time.sleep(1)

        # „É°„Çø„Éá„Éº„ÇøÁîüÊàê
        metadata = {
            'lastUpdated': datetime.now().isoformat(),
            'totalFeeds': len(successful_feeds),
            'feeds': successful_feeds
        }

        with open('data/feeds-meta.json', 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

        print(f"\nüìä ÊúÄÁµÇÁµêÊûú: {len(successful_feeds)}ÂÄã„ÅÆ„Éï„Ç£„Éº„Éâ„ÅåÊ≠£Â∏∏„Å´ÂèñÂæó„Åï„Çå„Åæ„Åó„Åü")
        
        if len(successful_feeds) == 0:
            print("‚ùå ÂÖ®„Å¶„ÅÆ„Éï„Ç£„Éº„Éâ„ÅÆÂèñÂæó„Å´Â§±Êïó„Åó„Åæ„Åó„Åü")
            exit(1)
        
        EOF

    - name: Validate Generated Files
      run: |
        echo "=== ÁîüÊàê„Éï„Ç°„Ç§„É´Ê§úË®º ==="
        for file in data/*.json; do
          if [ -f "$file" ]; then
            if jq empty "$file" 2>/dev/null; then
              entries=$(jq '.entries | length' "$file")
              title=$(jq -r '.title' "$file")
              echo "‚úÖ $(basename "$file"): ${entries}‰ª∂ - ${title}"
            else
              echo "‚ùå $(basename "$file"): JSONÁÑ°Âäπ"
            fi
          fi
        done

    - name: Commit and Push
      uses: stefanzweifel/git-auto-commit-action@v4
      with:
        commit_message: 'RSS feeds updated via custom script - $(date -u +"%Y-%m-%d %H:%M UTC")'
        file_pattern: 'data/*.json'
